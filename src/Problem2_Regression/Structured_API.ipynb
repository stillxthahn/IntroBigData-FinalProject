{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T09:19:50.934133Z",
     "iopub.status.busy": "2025-04-11T09:19:50.933623Z",
     "iopub.status.idle": "2025-04-11T09:19:59.496320Z",
     "shell.execute_reply": "2025-04-11T09:19:59.494993Z",
     "shell.execute_reply.started": "2025-04-11T09:19:50.934084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, hour, dayofweek\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import math\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Taxi Trip Duration Analysis and Prediction\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data and Initial Exploration\n",
    "We load the `train.csv` file, which contains the NYC Taxi Trip Duration dataset with features like pickup/dropoff locations, timestamps, and the target variable `trip_duration`. We display the schema to check column names and data types, and show a sample of the data to understand its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T09:19:59.498610Z",
     "iopub.status.busy": "2025-04-11T09:19:59.497871Z",
     "iopub.status.idle": "2025-04-11T09:20:15.429256Z",
     "shell.execute_reply": "2025-04-11T09:20:15.428252Z",
     "shell.execute_reply.started": "2025-04-11T09:19:59.498573Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of the training dataset:\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- vendor_id: integer (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- trip_duration: integer (nullable = true)\n",
      "\n",
      "\n",
      "Sample of the training dataset:\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "|       id|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|  pickup_longitude|   pickup_latitude| dropoff_longitude|  dropoff_latitude|store_and_fwd_flag|trip_duration|\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "|id2875421|        2|2016-03-14 17:24:55|2016-03-14 17:32:30|              1| -73.9821548461914| 40.76793670654297|-73.96463012695312|40.765602111816406|                 N|          455|\n",
      "|id2377394|        1|2016-06-12 00:43:35|2016-06-12 00:54:38|              1|-73.98041534423828|40.738563537597656|-73.99948120117188| 40.73115158081055|                 N|          663|\n",
      "|id3858529|        2|2016-01-19 11:35:24|2016-01-19 12:10:48|              1| -73.9790267944336|40.763938903808594|-74.00533294677734|40.710086822509766|                 N|         2124|\n",
      "|id3504673|        2|2016-04-06 19:32:31|2016-04-06 19:39:40|              1|-74.01004028320312|   40.719970703125|-74.01226806640625| 40.70671844482422|                 N|          429|\n",
      "|id2181028|        2|2016-03-26 13:30:55|2016-03-26 13:38:10|              1|-73.97305297851562|40.793209075927734| -73.9729232788086| 40.78252029418945|                 N|          435|\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "train_data = spark.read.csv(\"/kaggle/input/problem02/train.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Check schema and sample data\n",
    "print(\"Schema of the training dataset:\")\n",
    "train_data.printSchema()\n",
    "\n",
    "print(\"\\nSample of the training dataset:\")\n",
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Summary Statistics and Missing Values\n",
    "We compute summary statistics for numerical columns to understand their distributions and identify potential outliers. We also check for missing values in each column to ensure data quality, which is crucial for model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T09:20:15.431349Z",
     "iopub.status.busy": "2025-04-11T09:20:15.430956Z",
     "iopub.status.idle": "2025-04-11T09:20:40.858520Z",
     "shell.execute_reply": "2025-04-11T09:20:40.857441Z",
     "shell.execute_reply.started": "2025-04-11T09:20:15.431315Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary statistics for numerical columns in train.csv:\n",
      "+-------+------------------+-------------------+-------------------+-------------------+-------------------+-----------------+\n",
      "|summary|   passenger_count|   pickup_longitude|    pickup_latitude|  dropoff_longitude|   dropoff_latitude|    trip_duration|\n",
      "+-------+------------------+-------------------+-------------------+-------------------+-------------------+-----------------+\n",
      "|  count|           1458644|            1458644|            1458644|            1458644|            1458644|          1458644|\n",
      "|   mean|1.6645295219395548| -73.97348630489282| 40.750920908391734|  -73.9734159469458|   40.7517995149002|959.4922729603659|\n",
      "| stddev|  1.31424216782312| 0.0709018584227037|0.03288118625763338| 0.0706432680972028|0.03589055560563534|5237.431724497609|\n",
      "|    min|                 0|-121.93334197998047|  34.35969543457031|-121.93330383300781|   32.1811408996582|                1|\n",
      "|    max|                 9| -61.33552932739258|  51.88108444213867| -61.33552932739258|  43.92102813720703|          3526282|\n",
      "+-------+------------------+-------------------+-------------------+-------------------+-------------------+-----------------+\n",
      "\n",
      "\n",
      "Missing values in each column in train.csv:\n",
      "id: 0\n",
      "vendor_id: 0\n",
      "pickup_datetime: 0\n",
      "dropoff_datetime: 0\n",
      "passenger_count: 0\n",
      "pickup_longitude: 0\n",
      "pickup_latitude: 0\n",
      "dropoff_longitude: 0\n",
      "dropoff_latitude: 0\n",
      "store_and_fwd_flag: 0\n",
      "trip_duration: 0\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics for numerical columns\n",
    "numerical_cols = [\"passenger_count\", \"pickup_longitude\", \"pickup_latitude\",\n",
    "                  \"dropoff_longitude\", \"dropoff_latitude\", \"trip_duration\"]\n",
    "print(\"\\nSummary statistics for numerical columns in train.csv:\")\n",
    "train_data.select(numerical_cols).describe().show()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column in train.csv:\")\n",
    "for column in train_data.columns:\n",
    "    missing_count = train_data.filter(col(column).isNull()).count()\n",
    "    print(f\"{column}: {missing_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Feature Distribution Analysis\n",
    "We explore the distributions of key features to guide preprocessing:\n",
    "- `trip_duration`: To identify outliers in the target variable.\n",
    "- `passenger_count`: To understand the distribution of passengers per trip.\n",
    "- `store_and_fwd_flag`: To check the balance of this categorical feature.\n",
    "These insights help us decide on outlier removal and feature encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T09:20:40.860411Z",
     "iopub.status.busy": "2025-04-11T09:20:40.860001Z",
     "iopub.status.idle": "2025-04-11T09:20:48.846244Z",
     "shell.execute_reply": "2025-04-11T09:20:48.845095Z",
     "shell.execute_reply.started": "2025-04-11T09:20:40.860370Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trip duration distribution (quantiles) in train.csv:\n",
      "+-------+-------------+\n",
      "|summary|trip_duration|\n",
      "+-------+-------------+\n",
      "|    min|            1|\n",
      "|    25%|          397|\n",
      "|    50%|          662|\n",
      "|    75%|         1075|\n",
      "|    max|      3526282|\n",
      "+-------+-------------+\n",
      "\n",
      "\n",
      "Passenger count distribution in train.csv:\n",
      "+---------------+-------+\n",
      "|passenger_count|  count|\n",
      "+---------------+-------+\n",
      "|              0|     60|\n",
      "|              1|1033540|\n",
      "|              2| 210318|\n",
      "|              3|  59896|\n",
      "|              4|  28404|\n",
      "|              5|  78088|\n",
      "|              6|  48333|\n",
      "|              7|      3|\n",
      "|              8|      1|\n",
      "|              9|      1|\n",
      "+---------------+-------+\n",
      "\n",
      "\n",
      "Store and forward flag distribution in train.csv:\n",
      "+------------------+-------+\n",
      "|store_and_fwd_flag|  count|\n",
      "+------------------+-------+\n",
      "|                 Y|   8045|\n",
      "|                 N|1450599|\n",
      "+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Distribution of trip_duration\n",
    "print(\"\\nTrip duration distribution (quantiles) in train.csv:\")\n",
    "train_data.select(\"trip_duration\").summary(\"min\", \"25%\", \"50%\", \"75%\", \"max\").show()\n",
    "\n",
    "# Distribution of passenger_count\n",
    "print(\"\\nPassenger count distribution in train.csv:\")\n",
    "train_data.groupBy(\"passenger_count\").count().orderBy(\"passenger_count\").show()\n",
    "\n",
    "# Distribution of store_and_fwd_flag\n",
    "print(\"\\nStore and forward flag distribution in train.csv:\")\n",
    "train_data.groupBy(\"store_and_fwd_flag\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering: Temporal Features and Distance\n",
    "We engineer new features to improve model performance:\n",
    "- Extract `pickup_hour` and `pickup_dayofweek` from `pickup_datetime` to capture temporal patterns (e.g., rush hour effects).\n",
    "- Calculate `distance_km` between pickup and dropoff locations using the Haversine formula, which computes the great-circle distance in kilometers. This feature is likely a strong predictor of trip duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T09:20:48.847638Z",
     "iopub.status.busy": "2025-04-11T09:20:48.847236Z",
     "iopub.status.idle": "2025-04-11T09:20:50.704884Z",
     "shell.execute_reply": "2025-04-11T09:20:50.703655Z",
     "shell.execute_reply.started": "2025-04-11T09:20:48.847597Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample with new temporal features in train.csv:\n",
      "+-------------------+-----------+----------------+\n",
      "|    pickup_datetime|pickup_hour|pickup_dayofweek|\n",
      "+-------------------+-----------+----------------+\n",
      "|2016-03-14 17:24:55|         17|               2|\n",
      "|2016-06-12 00:43:35|          0|               1|\n",
      "|2016-01-19 11:35:24|         11|               3|\n",
      "|2016-04-06 19:32:31|         19|               4|\n",
      "|2016-03-26 13:30:55|         13|               7|\n",
      "+-------------------+-----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Sample with distance feature in train.csv:\n",
      "+------------------+------------------+------------------+------------------+------------------+\n",
      "|  pickup_longitude|   pickup_latitude| dropoff_longitude|  dropoff_latitude|       distance_km|\n",
      "+------------------+------------------+------------------+------------------+------------------+\n",
      "| -73.9821548461914| 40.76793670654297|-73.96463012695312|40.765602111816406|1.4985207796474773|\n",
      "|-73.98041534423828|40.738563537597656|-73.99948120117188| 40.73115158081055| 1.805507168795824|\n",
      "| -73.9790267944336|40.763938903808594|-74.00533294677734|40.710086822509766|  6.38509849525294|\n",
      "|-74.01004028320312|   40.719970703125|-74.01226806640625| 40.70671844482422| 1.485498422771006|\n",
      "|-73.97305297851562|40.793209075927734| -73.9729232788086| 40.78252029418945| 1.188588459334221|\n",
      "+------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract hour and day of week from pickup_datetime\n",
    "train_data = train_data.withColumn(\"pickup_datetime\", to_timestamp(col(\"pickup_datetime\")))\n",
    "train_data = train_data.withColumn(\"pickup_hour\", hour(col(\"pickup_datetime\")))\n",
    "train_data = train_data.withColumn(\"pickup_dayofweek\", dayofweek(col(\"pickup_datetime\")))\n",
    "\n",
    "print(\"\\nSample with new temporal features in train.csv:\")\n",
    "train_data.select(\"pickup_datetime\", \"pickup_hour\", \"pickup_dayofweek\").show(5)\n",
    "\n",
    "# Calculate distance between pickup and dropoff (Haversine formula)\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    R = 6371  # Earth radius in kilometers\n",
    "    dlat = math.radians(lat2 - lat1)\n",
    "    dlon = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dlat/2)**2 + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "# Register UDF for distance calculation\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "haversine_udf = udf(haversine, DoubleType())\n",
    "train_data = train_data.withColumn(\"distance_km\",\n",
    "    haversine_udf(col(\"pickup_longitude\"), col(\"pickup_latitude\"),\n",
    "                  col(\"dropoff_longitude\"), col(\"dropoff_latitude\")))\n",
    "\n",
    "print(\"\\nSample with distance feature in train.csv:\")\n",
    "train_data.select(\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"distance_km\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Training Data\n",
    "We preprocess the training data to ensure it is suitable for model training:\n",
    "- Encode the categorical `store_and_fwd_flag` using `StringIndexer`.\n",
    "- Drop rows with missing values.\n",
    "- Filter `trip_duration` to a reasonable range (60 seconds to 4 hours) to remove outliers.\n",
    "- Filter coordinates to NYC bounds (latitude: 40-41, longitude: -74 to -73) to exclude invalid locations.\n",
    "These steps improve data quality and consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T09:20:50.706485Z",
     "iopub.status.busy": "2025-04-11T09:20:50.706101Z",
     "iopub.status.idle": "2025-04-11T09:20:53.238122Z",
     "shell.execute_reply": "2025-04-11T09:20:53.237017Z",
     "shell.execute_reply.started": "2025-04-11T09:20:50.706451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Encode store_and_fwd_flag\n",
    "indexer = StringIndexer(inputCol=\"store_and_fwd_flag\", outputCol=\"store_and_fwd_flag_indexed\")\n",
    "train_data = indexer.fit(train_data).transform(train_data)\n",
    "\n",
    "# Handle missing values\n",
    "train_data = train_data.na.drop()\n",
    "\n",
    "# Filter outliers in trip_duration (e.g., keep trips between 60 seconds and 4 hours)\n",
    "train_data = train_data.filter((col(\"trip_duration\") >= 60) & (col(\"trip_duration\") <= 14400))\n",
    "\n",
    "# Filter reasonable coordinates (NYC bounds)\n",
    "train_data = train_data.filter((col(\"pickup_latitude\").between(40, 41)) &\n",
    "                               (col(\"pickup_longitude\").between(-74, -73)) &\n",
    "                               (col(\"dropoff_latitude\").between(40, 41)) &\n",
    "                               (col(\"dropoff_longitude\").between(-74, -73)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection, Vector Assembly, and Train-Test Split\n",
    "We select features for the model, including numerical features, engineered features (`pickup_hour`, `pickup_dayofweek`, `distance_km`), and the encoded `store_and_fwd_flag`. We use `VectorAssembler` to combine these features into a single vector column. The data is then split into training and validation sets (80-20 split) for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T09:20:53.239073Z",
     "iopub.status.busy": "2025-04-11T09:20:53.238750Z",
     "iopub.status.idle": "2025-04-11T09:20:53.392808Z",
     "shell.execute_reply": "2025-04-11T09:20:53.391661Z",
     "shell.execute_reply.started": "2025-04-11T09:20:53.239046Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Feature selection and vector assembly for training\n",
    "feature_cols = [\"vendor_id\", \"passenger_count\", \"pickup_longitude\", \"pickup_latitude\",\n",
    "                \"dropoff_longitude\", \"dropoff_latitude\", \"pickup_hour\",\n",
    "                \"pickup_dayofweek\", \"distance_km\", \"store_and_fwd_flag_indexed\"]\n",
    "\n",
    "# Assemble features into a vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "assembled_train_data = assembler.transform(train_data).select(\"features\", \"trip_duration\", \"id\")\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_split, val_split = assembled_train_data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Analyze Decision Tree Regressor\n",
    "We train a Decision Tree Regressor with a maximum depth of 5 and variance as the impurity measure. After training on the training split, we analyze the model by printing its tree structure and feature importances to understand its decision rules and the most influential features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T09:20:53.396138Z",
     "iopub.status.busy": "2025-04-11T09:20:53.395644Z",
     "iopub.status.idle": "2025-04-11T09:21:41.978170Z",
     "shell.execute_reply": "2025-04-11T09:21:41.974926Z",
     "shell.execute_reply.started": "2025-04-11T09:20:53.396092Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tree Structure:\n",
      "DecisionTreeRegressionModel: uid=DecisionTreeRegressor_adc738a0fa5f, depth=5, numNodes=63, numFeatures=10\n",
      "  If (feature 8 <= 5.423026623607559)\n",
      "   If (feature 8 <= 1.9973042790322402)\n",
      "    If (feature 8 <= 1.2045349743906306)\n",
      "     If (feature 5 <= 40.767473220825195)\n",
      "      If (feature 6 <= 7.5)\n",
      "       Predict: 292.7657350433687\n",
      "      Else (feature 6 > 7.5)\n",
      "       Predict: 423.01783888279334\n",
      "     Else (feature 5 > 40.767473220825195)\n",
      "      If (feature 8 <= 0.9413700670582281)\n",
      "       Predict: 271.28365300017595\n",
      "      Else (feature 8 > 0.9413700670582281)\n",
      "       Predict: 335.8212076121858\n",
      "    Else (feature 8 > 1.2045349743906306)\n",
      "     If (feature 5 <= 40.769575119018555)\n",
      "      If (feature 6 <= 7.5)\n",
      "       Predict: 432.2597942073171\n",
      "      Else (feature 6 > 7.5)\n",
      "       Predict: 641.2321411512261\n",
      "     Else (feature 5 > 40.769575119018555)\n",
      "      If (feature 8 <= 1.6655255165486342)\n",
      "       Predict: 413.8078877262366\n",
      "      Else (feature 8 > 1.6655255165486342)\n",
      "       Predict: 520.8166480122704\n",
      "   Else (feature 8 > 1.9973042790322402)\n",
      "    If (feature 8 <= 3.0711013161184373)\n",
      "     If (feature 6 <= 7.5)\n",
      "      If (feature 6 <= 6.5)\n",
      "       Predict: 557.4558618630573\n",
      "      Else (feature 6 > 6.5)\n",
      "       Predict: 671.53310626703\n",
      "     Else (feature 6 > 7.5)\n",
      "      If (feature 5 <= 40.7740364074707)\n",
      "       Predict: 862.2270865409755\n",
      "      Else (feature 5 > 40.7740364074707)\n",
      "       Predict: 675.5218258796049\n",
      "    Else (feature 8 > 3.0711013161184373)\n",
      "     If (feature 6 <= 7.5)\n",
      "      If (feature 8 <= 4.146281383341534)\n",
      "       Predict: 751.3429458475133\n",
      "      Else (feature 8 > 4.146281383341534)\n",
      "       Predict: 912.2203389830509\n",
      "     Else (feature 6 > 7.5)\n",
      "      If (feature 6 <= 18.5)\n",
      "       Predict: 1198.4844088553932\n",
      "      Else (feature 6 > 18.5)\n",
      "       Predict: 976.2883389410306\n",
      "  Else (feature 8 > 5.423026623607559)\n",
      "   If (feature 8 <= 14.745358015340809)\n",
      "    If (feature 8 <= 8.309506701788134)\n",
      "     If (feature 6 <= 6.5)\n",
      "      If (feature 5 <= 40.713579177856445)\n",
      "       Predict: 1293.4656849124756\n",
      "      Else (feature 5 > 40.713579177856445)\n",
      "       Predict: 1074.6170938594873\n",
      "     Else (feature 6 > 6.5)\n",
      "      If (feature 6 <= 19.5)\n",
      "       Predict: 1533.0096183817964\n",
      "      Else (feature 6 > 19.5)\n",
      "       Predict: 1320.4774491563483\n",
      "    Else (feature 8 > 8.309506701788134)\n",
      "     If (feature 6 <= 6.5)\n",
      "      If (feature 5 <= 40.713579177856445)\n",
      "       Predict: 1739.8857041755132\n",
      "      Else (feature 5 > 40.713579177856445)\n",
      "       Predict: 1332.986198243413\n",
      "     Else (feature 6 > 6.5)\n",
      "      If (feature 6 <= 18.5)\n",
      "       Predict: 2077.10463619986\n",
      "      Else (feature 6 > 18.5)\n",
      "       Predict: 1627.0482939279957\n",
      "   Else (feature 8 > 14.745358015340809)\n",
      "    If (feature 6 <= 5.5)\n",
      "     If (feature 8 <= 24.72663914492781)\n",
      "      If (feature 4 <= -73.95444107055664)\n",
      "       Predict: 1880.9809674861222\n",
      "      Else (feature 4 > -73.95444107055664)\n",
      "       Predict: 1747.2342422887796\n",
      "     Else (feature 8 > 24.72663914492781)\n",
      "      If (feature 2 <= -73.97210311889648)\n",
      "       Predict: 2625.766666666667\n",
      "      Else (feature 2 > -73.97210311889648)\n",
      "       Predict: 2011.5243902439024\n",
      "    Else (feature 6 > 5.5)\n",
      "     If (feature 6 <= 18.5)\n",
      "      If (feature 6 <= 13.5)\n",
      "       Predict: 2735.012253233492\n",
      "      Else (feature 6 > 13.5)\n",
      "       Predict: 3279.237998935604\n",
      "     Else (feature 6 > 18.5)\n",
      "      If (feature 6 <= 19.5)\n",
      "       Predict: 2499.860419397117\n",
      "      Else (feature 6 > 19.5)\n",
      "       Predict: 2155.412087912088\n",
      "\n",
      "\n",
      "Feature Importances:\n",
      "vendor_id: 0.0000\n",
      "passenger_count: 0.0000\n",
      "pickup_longitude: 0.0000\n",
      "pickup_latitude: 0.0000\n",
      "dropoff_longitude: 0.0001\n",
      "dropoff_latitude: 0.0118\n",
      "pickup_hour: 0.0733\n",
      "pickup_dayofweek: 0.0000\n",
      "distance_km: 0.9148\n",
      "store_and_fwd_flag_indexed: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Train DecisionTreeRegressor model\n",
    "dt = DecisionTreeRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"trip_duration\",\n",
    "    maxDepth=5,          # Control tree complexity\n",
    "    impurity=\"variance\", # Use variance for regression\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Fit the model on the training split\n",
    "model = dt.fit(train_split)\n",
    "\n",
    "# Analyze tree structure and feature importance\n",
    "print(\"\\nTree Structure:\")\n",
    "print(model.toDebugString)\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "for feature, importance in zip(feature_cols, model.featureImportances.toArray()):\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model on Validation Split\n",
    "We evaluate the model on the validation split using:\n",
    "- **Root Mean Squared Error (RMSE)**: Measures the average prediction error in seconds.\n",
    "- **R-squared (R²)**: Indicates the proportion of variance in `trip_duration` explained by the model.\n",
    "These metrics provide an estimate of the model's performance on unseen data from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T09:21:41.982538Z",
     "iopub.status.busy": "2025-04-11T09:21:41.979480Z",
     "iopub.status.idle": "2025-04-11T09:22:03.513605Z",
     "shell.execute_reply": "2025-04-11T09:22:03.512359Z",
     "shell.execute_reply.started": "2025-04-11T09:21:41.982488Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Root Mean Squared Error (RMSE) on validation split: 385.6099\n",
      "R-squared (R²) on validation split: 0.6620\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation split\n",
    "val_predictions = model.transform(val_split)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"trip_duration\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "rmse = rmse_evaluator.evaluate(val_predictions)\n",
    "print(f\"\\nRoot Mean Squared Error (RMSE) on validation split: {rmse:.4f}\")\n",
    "\n",
    "# Calculate R²\n",
    "r2_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"trip_duration\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "r2 = r2_evaluator.evaluate(val_predictions)\n",
    "print(f\"R-squared (R²) on validation split: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Test Data, Generate Predictions, and Cleanup\n",
    "We load and preprocess `test.csv` to match the training data format by adding temporal features, calculating `distance_km`, encoding `store_and_fwd_flag`, and filtering coordinates. We then generate predictions and save them to `submission.csv` in the Kaggle submission format (`id`, `trip_duration`). Finally, we stop the Spark session to free up resources.\n",
    "\n",
    "**Note**: `test.csv` lacks `trip_duration`, so we cannot evaluate directly. Submit `submission.csv` to Kaggle to get the test set performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T09:22:03.515487Z",
     "iopub.status.busy": "2025-04-11T09:22:03.515003Z",
     "iopub.status.idle": "2025-04-11T09:22:13.383973Z",
     "shell.execute_reply": "2025-04-11T09:22:13.382465Z",
     "shell.execute_reply.started": "2025-04-11T09:22:03.515450Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined predictions have been saved to submission_combined.csv\n",
      "\n",
      "Sample Combined Predictions:\n",
      "+---------+------------------+\n",
      "|id       |trip_duration     |\n",
      "+---------+------------------+\n",
      "|id3004672|862.2270865409755 |\n",
      "|id3505355|862.2270865409755 |\n",
      "|id1217141|641.2321411512261 |\n",
      "|id2150126|976.2883389410306 |\n",
      "|id1598245|423.01783888279334|\n",
      "+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Load the test data\n",
    "test_data = spark.read.csv(\"/kaggle/input/problem02/test.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Preprocess the test data in the same way as the training data\n",
    "# Add temporal features\n",
    "test_data = test_data.withColumn(\"pickup_datetime\", to_timestamp(col(\"pickup_datetime\")))\n",
    "test_data = test_data.withColumn(\"pickup_hour\", hour(col(\"pickup_datetime\")))\n",
    "test_data = test_data.withColumn(\"pickup_dayofweek\", dayofweek(col(\"pickup_datetime\")))\n",
    "\n",
    "# Calculate distance (assumes haversine_udf is defined earlier)\n",
    "test_data = test_data.withColumn(\"distance_km\",\n",
    "    haversine_udf(col(\"pickup_longitude\"), col(\"pickup_latitude\"),\n",
    "                  col(\"dropoff_longitude\"), col(\"dropoff_latitude\")))\n",
    "\n",
    "# Encode store_and_fwd_flag (assumes indexer is defined earlier)\n",
    "test_data = indexer.fit(test_data).transform(test_data)\n",
    "\n",
    "# Handle missing values and filter coordinates\n",
    "test_data = test_data.na.drop()\n",
    "test_data = test_data.filter((col(\"pickup_latitude\").between(40, 41)) &\n",
    "                             (col(\"pickup_longitude\").between(-74, -73)) &\n",
    "                             (col(\"dropoff_latitude\").between(40, 41)) &\n",
    "                             (col(\"dropoff_longitude\").between(-74, -73)))\n",
    "\n",
    "# Assemble features for test data (assumes assembler is defined earlier)\n",
    "assembled_test_data = assembler.transform(test_data).select(\"features\", \"id\")\n",
    "\n",
    "# Make predictions on test data (assumes model is defined earlier)\n",
    "test_predictions = model.transform(assembled_test_data)\n",
    "\n",
    "# Prepare MLlib predictions\n",
    "mllib_predictions = test_predictions.select(\"id\", col(\"prediction\").alias(\"trip_duration\"))\n",
    "# Combine the predictions by joining on the 'id' column\n",
    "combined_predictions = mllib_predictions\n",
    "\n",
    "# Write the combined predictions to a temporary directory with a single partition\n",
    "temp_dir = \"temp_submission_combined\"\n",
    "combined_predictions.coalesce(1).write.csv(temp_dir, header=True, mode=\"overwrite\")\n",
    "\n",
    "# Find the single CSV file in the temporary directory\n",
    "csv_file = [f for f in os.listdir(temp_dir) if f.startswith(\"part-\") and f.endswith(\".csv\")][0]\n",
    "csv_file_path = os.path.join(temp_dir, csv_file)\n",
    "\n",
    "# Move and rename the CSV file to submission_combined.csv\n",
    "final_submission_path = \"submission_combined.csv\"\n",
    "shutil.move(csv_file_path, final_submission_path)\n",
    "\n",
    "# Clean up the temporary directory\n",
    "shutil.rmtree(temp_dir)\n",
    "\n",
    "print(f\"Combined predictions have been saved to {final_submission_path}\")\n",
    "\n",
    "# Print sample combined predictions\n",
    "print(\"\\nSample Combined Predictions:\")\n",
    "combined_predictions.show(5, truncate=False)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 44258,
     "sourceId": 6960,
     "sourceType": "competition"
    },
    {
     "datasetId": 7106949,
     "sourceId": 11356177,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
