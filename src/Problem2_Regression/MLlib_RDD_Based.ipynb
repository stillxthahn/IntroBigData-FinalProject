{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-10T17:23:00.712025Z",
     "iopub.status.busy": "2025-04-10T17:23:00.711724Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/10 17:23:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/10 17:23:40 WARN DAGScheduler: Broadcasting large task binary with size 1475.1 KiB            \n",
      "25/04/10 17:23:42 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB               \n",
      "25/04/10 17:23:44 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB               \n",
      "25/04/10 17:23:46 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB               \n",
      "25/04/10 17:23:48 WARN DAGScheduler: Broadcasting large task binary with size 1108.3 KiB(5 + 1) / 6]\n",
      "25/04/10 17:23:50 WARN DAGScheduler: Broadcasting large task binary with size 8.2 MiB               \n",
      "25/04/10 17:23:53 WARN DAGScheduler: Broadcasting large task binary with size 1472.7 KiB(5 + 1) / 6]\n",
      "25/04/10 17:23:55 WARN DAGScheduler: Broadcasting large task binary with size 11.6 MiB              \n",
      "25/04/10 17:23:59 WARN DAGScheduler: Broadcasting large task binary with size 1905.8 KiB(4 + 2) / 6]\n",
      "25/04/10 17:24:02 WARN DAGScheduler: Broadcasting large task binary with size 15.9 MiB              \n",
      "25/04/10 17:24:08 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB   (4 + 2) / 6]\n",
      "25/04/10 17:24:13 WARN DAGScheduler: Broadcasting large task binary with size 18.3 MiB  (0 + 0) / 6]\n",
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Mean Squared Error = 26930410.005425915\n",
      "Validation Root Mean Squared Error = 5189.451802013958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/10 17:24:39 WARN DAGScheduler: Broadcasting large task binary with size 18.3 MiB\n",
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 predictions for test.csv:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/10 17:24:44 WARN DAGScheduler: Broadcasting large task binary with size 18.3 MiB\n",
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "831.7209302325581\n",
      "656.0\n",
      "565.1875\n",
      "1120.7688524590164\n",
      "388.34133653461384\n",
      "Learned regression tree model:\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "\n",
    "# Initialize SparkContext\n",
    "spark = SparkSession.builder.appName(\"RDDDecisionTreeRegression\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load CSV files as RDDs\n",
    "train_rdd_raw = sc.textFile(\"/kaggle/input/vuiver/train.csv\")\n",
    "test_rdd_raw = sc.textFile(\"/kaggle/input/vuiver/test.csv\")\n",
    "\n",
    "# Define column indices based on train.csv structure\n",
    "\n",
    "feature_indices = [1, 4, 5, 6, 7, 8]  # vendor_id, passenger_count, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude\n",
    "label_index = 10  # trip_duration\n",
    "\n",
    "# Function to parse train.csv into LabeledPoint\n",
    "def parse_train_line(line):\n",
    "    values = line.split(',')\n",
    "    try:\n",
    "        # Extract numeric features and label\n",
    "        features = [float(values[i]) for i in feature_indices]\n",
    "        label = float(values[label_index])\n",
    "        return LabeledPoint(label, features)\n",
    "    except (ValueError, IndexError) as e:\n",
    "        print(f\"Skipping invalid train row: {line} due to {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to parse test.csv into features only (no label)\n",
    "def parse_test_line(line):\n",
    "    values = line.split(',')\n",
    "    try:\n",
    "        # Extract numeric features (adjust indices for test.csv)\n",
    "        test_feature_indices = [1, 3, 4, 5, 6, 7]  # vendor_id, passenger_count, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude\n",
    "        features = [float(values[i]) for i in test_feature_indices]\n",
    "        return features  # Return features only, no LabeledPoint since no label\n",
    "    except (ValueError, IndexError) as e:\n",
    "        print(f\"Skipping invalid test row: {line} due to {e}\")\n",
    "        return None\n",
    "\n",
    "# Skip header and parse data\n",
    "header_train = train_rdd_raw.first()\n",
    "header_test = test_rdd_raw.first()\n",
    "train_rdd = train_rdd_raw.filter(lambda line: line != header_train).map(parse_train_line).filter(lambda x: x is not None)\n",
    "test_rdd = test_rdd_raw.filter(lambda line: line != header_test).map(parse_test_line).filter(lambda x: x is not None)\n",
    "\n",
    "# Split train_rdd into training and validation sets (since test.csv has no labels)\n",
    "(trainingData, validationData) = train_rdd.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model\n",
    "model = DecisionTree.trainRegressor(\n",
    "    trainingData,\n",
    "    categoricalFeaturesInfo={},  # Assume all features are continuous\n",
    "    impurity='variance',\n",
    "    maxDepth=20,\n",
    "    maxBins=32\n",
    ")\n",
    "\n",
    "# Evaluate model on validation set (from train.csv split)\n",
    "predictions = model.predict(validationData.map(lambda x: x.features))\n",
    "labelsAndPredictions = validationData.map(lambda lp: lp.label).zip(predictions)\n",
    "testMSE = labelsAndPredictions.map(lambda lp: (lp[0] - lp[1]) * (lp[0] - lp[1])).sum() / float(validationData.count())\n",
    "testRMSE = testMSE ** 0.5\n",
    "print('Validation Mean Squared Error = ' + str(testMSE))\n",
    "print('Validation Root Mean Squared Error = ' + str(testRMSE))\n",
    "\n",
    "# Predict on test.csv (no labels, just predictions)\n",
    "test_predictions = model.predict(test_rdd)\n",
    "test_predictions.take(5)  # View first 5 predictions\n",
    "print(\"First 5 predictions for test.csv:\")\n",
    "for pred in test_predictions.take(5):\n",
    "    print(pred)\n",
    "\n",
    "# Print the learned model\n",
    "print('Learned regression tree model:')\n",
    "print(model.toDebugString())\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, \"target/tmp/myDecisionTreeRegressionModel\")\n",
    "sameModel = DecisionTreeModel.load(sc, \"target/tmp/myDecisionTreeRegressionModel\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7071854,
     "sourceId": 11307779,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7105173,
     "sourceId": 11353998,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
