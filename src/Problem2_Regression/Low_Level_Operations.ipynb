{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0f2c5c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-12T09:08:14.834052Z",
     "iopub.status.busy": "2025-04-12T09:08:14.833790Z",
     "iopub.status.idle": "2025-04-12T10:20:50.680100Z",
     "shell.execute_reply": "2025-04-12T10:20:50.679382Z"
    },
    "papermill": {
     "duration": 4355.851361,
     "end_time": "2025-04-12T10:20:50.681774",
     "exception": false,
     "start_time": "2025-04-12T09:08:14.830413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/12 09:08:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth 0, count 79374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth 1, count 75697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth 2, count 73735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth 2, count 1962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth 1, count 3677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth 2, count 2298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth 2, count 1379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree model saved to decision_tree_model.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Mean Squared Error: 295578.5206743891\n",
      "Validation Root Mean Squared Error: 543.6713351597535\n",
      "Sample Predictions from test.csv:\n",
      "ID: id3004672, Features: [1.0, 1.0, -73.98812866210938, 40.73202896118164, -73.99017333984375, 40.7566795349121], Predicted trip_duration: 743.79\n",
      "ID: id3505355, Features: [1.0, 1.0, -73.96420288085938, 40.67999267578125, -73.95980834960938, 40.65540313720703], Predicted trip_duration: 743.79\n",
      "ID: id1217141, Features: [1.0, 1.0, -73.9974365234375, 40.73758316040039, -73.9861602783203, 40.729522705078125], Predicted trip_duration: 743.79\n",
      "ID: id2150126, Features: [2.0, 1.0, -73.95606994628906, 40.77190017700195, -73.98642730712889, 40.73046875], Predicted trip_duration: 743.79\n",
      "ID: id1598245, Features: [1.0, 1.0, -73.97021484375, 40.761474609375, -73.96150970458984, 40.755889892578125], Predicted trip_duration: 743.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import pickle\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Initialize SparkContext\n",
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    sc = SparkContext(\"local[*]\", \"ManualDecisionTree\")\n",
    "\n",
    "# Set high parallelism for large dataset\n",
    "sc._conf.set(\"spark.default.parallelism\", \"256\")\n",
    "\n",
    "# Load and parse the training data\n",
    "lines = sc.textFile(\"/kaggle/input/testmet/train.csv\", minPartitions=256)\n",
    "header = lines.first()\n",
    "data = lines.filter(lambda line: line != header)\n",
    "\n",
    "# Preprocessing parameters\n",
    "LONGITUDE_MIN, LONGITUDE_MAX = -74.3, -73.7\n",
    "LATITUDE_MIN, LATITUDE_MAX = 40.5, 41.0\n",
    "PASSENGER_MIN, PASSENGER_MAX = 1, 9\n",
    "TRIP_DURATION_MIN, TRIP_DURATION_MAX = 60, 36000  # 1 minute to 10 hours\n",
    "\n",
    "# Parse and preprocess training data \n",
    "def parse_train_line(line):\n",
    "    cols = line.split(\",\")\n",
    "    try:\n",
    "        vendor_id = float(cols[1])\n",
    "        passenger_count = float(cols[4])\n",
    "        pickup_longitude = float(cols[5])\n",
    "        pickup_latitude = float(cols[6])\n",
    "        dropoff_longitude = float(cols[7])\n",
    "        dropoff_latitude = float(cols[8])\n",
    "        trip_duration = float(cols[10])\n",
    "\n",
    "        # Filter outliers\n",
    "        if not (LONGITUDE_MIN <= pickup_longitude <= LONGITUDE_MAX and\n",
    "                LONGITUDE_MIN <= dropoff_longitude <= LONGITUDE_MAX and\n",
    "                LATITUDE_MIN <= pickup_latitude <= LATITUDE_MAX and\n",
    "                LATITUDE_MIN <= dropoff_latitude <= LATITUDE_MAX):\n",
    "            return None\n",
    "        if not (PASSENGER_MIN <= passenger_count <= PASSENGER_MAX):\n",
    "            return None\n",
    "        if not (TRIP_DURATION_MIN <= trip_duration <= TRIP_DURATION_MAX):\n",
    "            return None\n",
    "        if vendor_id not in (1, 2):\n",
    "            return None\n",
    "\n",
    "        features = [vendor_id, passenger_count, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude]\n",
    "        return (features, trip_duration)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "parsed_data = data.map(parse_train_line).filter(lambda x: x is not None).persist()\n",
    "\n",
    "# Compute feature min/max once\n",
    "num_features = 6\n",
    "feature_stats = []\n",
    "for i in range(num_features):\n",
    "    feature_rdd = parsed_data.map(lambda x: x[0][i])\n",
    "    min_val, max_val = feature_rdd.min(), feature_rdd.max()\n",
    "    feature_stats.append((min_val, max_val))\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_rdd, val_rdd = parsed_data.randomSplit([0.8, 0.2], seed=42)\n",
    "train_rdd.persist()\n",
    "val_rdd.persist()\n",
    "\n",
    "# Parse test data\n",
    "test_lines = sc.textFile(\"/kaggle/input/testmet/test.csv\", minPartitions=256)\n",
    "test_header = test_lines.first().strip()\n",
    "test_data = test_lines.filter(lambda line: line.strip() != test_header)\n",
    "\n",
    "def parse_test_line(line):\n",
    "    cols = line.split(\",\")\n",
    "    try:\n",
    "        id_val = cols[0]\n",
    "        vendor_id = float(cols[1])\n",
    "        passenger_count = float(cols[3])\n",
    "        pickup_longitude = float(cols[4])\n",
    "        pickup_latitude = float(cols[5])\n",
    "        dropoff_longitude = float(cols[6])\n",
    "        dropoff_latitude = float(cols[7])\n",
    "\n",
    "        # Filter outliers\n",
    "        if not (LONGITUDE_MIN <= pickup_longitude <= LONGITUDE_MAX and\n",
    "                LONGITUDE_MIN <= dropoff_longitude <= LONGITUDE_MAX and\n",
    "                LATITUDE_MIN <= pickup_latitude <= LATITUDE_MAX and\n",
    "                LATITUDE_MIN <= dropoff_latitude <= LATITUDE_MAX):\n",
    "            return None\n",
    "        if not (PASSENGER_MIN <= passenger_count <= PASSENGER_MAX):\n",
    "            return None\n",
    "        if vendor_id not in (1, 2):\n",
    "            return None\n",
    "\n",
    "        features = [vendor_id, passenger_count, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude]\n",
    "        return (id_val, features)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "test_data_parsed = test_data.map(parse_test_line).filter(lambda x: x is not None).persist()\n",
    "\n",
    "# Decision tree functions\n",
    "def calculate_mse_and_count(data_rdd):\n",
    "    zero_value = (0.0, 0.0, 0)\n",
    "    seq_op = lambda acc, x: (acc[0] + x[1], acc[1] + x[1]**2, acc[2] + 1)\n",
    "    comb_op = lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1], acc1[2] + acc2[2])\n",
    "    stats = data_rdd.aggregate(zero_value, seq_op, comb_op)\n",
    "    sum_y, sum_y2, count = stats\n",
    "    if count == 0:\n",
    "        return 0.0, 0\n",
    "    mean = sum_y / count\n",
    "    variance = (sum_y2 / count) - mean**2 if count > 0 else 0.0\n",
    "    return variance, count\n",
    "\n",
    "def find_best_split(data_rdd, feature_idx, min_val, max_val, num_thresholds=10):\n",
    "    if min_val >= max_val:\n",
    "        return None, float(\"inf\")\n",
    "\n",
    "    # Sample thresholds randomly for speed\n",
    "    def get_thresholds():\n",
    "        if max_val == min_val:\n",
    "            return [min_val]\n",
    "        return [min_val + random.random() * (max_val - min_val) for _ in range(num_thresholds)]\n",
    "\n",
    "    thresholds = get_thresholds()\n",
    "    total_count = data_rdd.count()\n",
    "    total_stats = data_rdd.aggregate(\n",
    "        (0.0, 0.0, 0),\n",
    "        lambda acc, x: (acc[0] + x[1], acc[1] + x[1]**2, acc[2] + 1),\n",
    "        lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1], acc1[2] + acc2[2])\n",
    "    )\n",
    "    total_sum_y, total_sum_y2, total_count_agg = total_stats\n",
    "    total_variance = (total_sum_y2 / total_count_agg - (total_sum_y / total_count_agg)**2) if total_count_agg > 0 else 0.0\n",
    "\n",
    "    best_threshold = None\n",
    "    best_mse = float(\"inf\")\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        left_stats = data_rdd.filter(lambda x: x[0][feature_idx] <= threshold).aggregate(\n",
    "            (0.0, 0.0, 0),\n",
    "            lambda acc, x: (acc[0] + x[1], acc[1] + x[1]**2, acc[2] + 1),\n",
    "            lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1], acc1[2] + acc2[2])\n",
    "        )\n",
    "        left_sum_y, left_sum_y2, left_count = left_stats\n",
    "        right_sum_y = total_sum_y - left_sum_y\n",
    "        right_sum_y2 = total_sum_y2 - left_sum_y2\n",
    "        right_count = total_count_agg - left_count\n",
    "\n",
    "        if left_count < 1000 or right_count < 1000:  # Increased min node size\n",
    "            continue\n",
    "\n",
    "        left_mse = (left_sum_y2 / left_count - (left_sum_y / left_count)**2) if left_count > 0 else 0\n",
    "        right_mse = (right_sum_y2 / right_count - (right_sum_y / right_count)**2) if right_count > 0 else 0\n",
    "        total_mse = (left_mse * left_count + right_mse * right_count) / total_count_agg\n",
    "\n",
    "        if total_mse < best_mse:\n",
    "            best_mse = total_mse\n",
    "            best_threshold = threshold\n",
    "\n",
    "    # Variance reduction check\n",
    "    if best_mse >= total_variance * 0.95:  # Looser threshold for speed\n",
    "        return None, float(\"inf\")\n",
    "\n",
    "    return best_threshold, best_mse\n",
    "\n",
    "def build_decision_tree(data_rdd, depth=0, max_depth=20):\n",
    "    count = data_rdd.count()\n",
    "    print(f\"Depth {depth}, count {count}\")\n",
    "\n",
    "    if count < 2000 or depth >= max_depth:  # Increased min total size\n",
    "        stats = data_rdd.aggregate(\n",
    "            (0.0, 0),\n",
    "            lambda acc, x: (acc[0] + x[1], acc[1] + 1),\n",
    "            lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
    "        )\n",
    "        sum_y, count = stats\n",
    "        return {\"leaf\": True, \"prediction\": sum_y / count if count > 0 else 0.0}\n",
    "\n",
    "    # Evaluate features sequentially to avoid RDD serialization\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "    best_mse = float(\"inf\")\n",
    "\n",
    "    for idx in range(num_features):\n",
    "        threshold, mse = find_best_split(data_rdd, idx, feature_stats[idx][0], feature_stats[idx][1])\n",
    "        if threshold is not None and mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_feature = idx\n",
    "            best_threshold = threshold\n",
    "\n",
    "    if best_feature is None:\n",
    "        stats = data_rdd.aggregate(\n",
    "            (0.0, 0),\n",
    "            lambda acc, x: (acc[0] + x[1], acc[1] + 1),\n",
    "            lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
    "        )\n",
    "        sum_y, count = stats\n",
    "        return {\"leaf\": True, \"prediction\": sum_y / count if count > 0 else 0.0}\n",
    "\n",
    "    left_rdd = data_rdd.filter(lambda x: x[0][best_feature] <= best_threshold).persist()\n",
    "    right_rdd = data_rdd.filter(lambda x: x[0][best_feature] > best_threshold).persist()\n",
    "    left_tree = build_decision_tree(left_rdd, depth + 1, max_depth)\n",
    "    right_tree = build_decision_tree(right_rdd, depth + 1, max_depth)\n",
    "    left_rdd.unpersist()\n",
    "    right_rdd.unpersist()\n",
    "\n",
    "    return {\n",
    "        \"leaf\": False,\n",
    "        \"feature\": best_feature,\n",
    "        \"threshold\": best_threshold,\n",
    "        \"left\": left_tree,\n",
    "        \"right\": right_tree\n",
    "    }\n",
    "\n",
    "# Build the decision tree\n",
    "tree = build_decision_tree(train_rdd, max_depth=4)\n",
    "\n",
    "# Save the tree\n",
    "with open(\"decision_tree_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tree, f)\n",
    "print(\"Decision tree model saved to decision_tree_model.pkl\")\n",
    "\n",
    "def predict(tree, features):\n",
    "    if tree[\"leaf\"]:\n",
    "        return tree[\"prediction\"]\n",
    "    if features[tree[\"feature\"]] <= tree[\"threshold\"]:\n",
    "        return predict(tree[\"left\"], features)\n",
    "    return predict(tree[\"right\"], features)\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_predictions = val_rdd.map(lambda x: (x[1], predict(tree, x[0])))\n",
    "val_mse = val_predictions.map(lambda lp: (lp[0] - lp[1])**2).mean()\n",
    "val_rmse = val_mse ** 0.5\n",
    "print(f\"Validation Mean Squared Error: {val_mse}\")\n",
    "print(f\"Validation Root Mean Squared Error: {val_rmse}\")\n",
    "\n",
    "# Sample predictions on test data\n",
    "sample_test = test_data_parsed.take(5)\n",
    "print(\"Sample Predictions from test.csv:\")\n",
    "for id_val, features in sample_test:\n",
    "    pred = predict(tree, features)\n",
    "    print(f\"ID: {id_val}, Features: {features}, Predicted trip_duration: {pred:.2f}\")\n",
    "\n",
    "# Save all test predictions\n",
    "test_predictions = test_data_parsed.map(lambda x: (x[0], predict(tree, x[1])))\n",
    "test_predictions.saveAsTextFile(\"test_predictions.txt\")\n",
    "\n",
    "# Clean up\n",
    "train_rdd.unpersist()\n",
    "val_rdd.unpersist()\n",
    "test_data_parsed.unpersist()\n",
    "parsed_data.unpersist()\n",
    "\n",
    "# Stop SparkContext\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7071854,
     "sourceId": 11307779,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7105173,
     "sourceId": 11353998,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7120484,
     "sourceId": 11373884,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7120493,
     "sourceId": 11373893,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7120587,
     "sourceId": 11374006,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4375.2569,
   "end_time": "2025-04-12T10:21:05.968423",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-12T09:08:10.711523",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
